<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="glioma_classification process"><meta name="keywords" content="python"><meta name="author" content="Wuli帅林林,undefined"><meta name="copyright" content="Wuli帅林林"><title>glioma_classification process【Welcome to linlin】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="/favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="/js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#glioma-classification"><span class="toc-number">1.</span> <span class="toc-text">glioma_classification</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Wuli帅林林</div><div class="author-info-description">不畏将来 不念过去</div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/yuehailin" target="_blank">GitHub<i class="icon-dot bg-color7"></i></a><a class="links-button button-hover" href="mailto:1738152865@qq.com" target="_blank">E-Mail<i class="icon-dot bg-color6"></i></a><a class="links-button button-hover" href="tencent://message/?uin=1738152865&amp;Site=&amp;Menu=yes" target="_blank">QQ<i class="icon-dot bg-color0"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">12</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">7</span></a><a class="author-info-articles-categories article-meta" href="/categories"><span class="pull-top">分类</span><span class="pull-bottom">6</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="title-name" href="/">Welcome to linlin</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">glioma_classification process</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2019-11-08 | 更新于 2019-11-08</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/glioma/">glioma</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/python/">python</a></div></div></div><div class="main-content"><h1 id="glioma-classification"><a href="#glioma-classification" class="headerlink" title="glioma_classification"></a>glioma_classification</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据加载</span></span><br><span class="line">import pandas as pd</span><br><span class="line">from pandas import ExcelWriter</span><br><span class="line">from pandas import ExcelFile</span><br><span class="line">import numpy as np</span><br><span class="line"><span class="comment">#pd.set_option('display.max_columns', None)</span></span><br><span class="line"><span class="comment">#T1_Data = pd.read_csv('E:\\Data_cjh\\BraTS2019\\test\\t1_radiomics_features.csv')E:\Data_cjh\BraTS2019\radimoic_feature\BraTS_2017</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#'E:\\Data_cjh\\BraTS2019\\Feature_brats_2019\\WTandET\\WT\\WT_t1_features.csv'</span></span><br><span class="line"><span class="comment">#T1_Data = pd.read_csv('G:\\MediclImage\\Data\\NeckFibrosis\\new\\T1_C_PreRT_ok.csv')</span></span><br><span class="line">Flair_Data = pd.read_csv(<span class="string">'flair_select_features.csv'</span>)</span><br><span class="line">T1_Data = pd.read_csv(<span class="string">'t1_select_features.csv'</span>)</span><br><span class="line">T1ce_Data = pd.read_csv(<span class="string">'t1ce_select_features.csv'</span>)</span><br><span class="line">T2_Data = pd.read_csv(<span class="string">'t2_select_features.csv'</span>)</span><br><span class="line"><span class="comment"># from sklearn.utils import shuffle</span></span><br><span class="line"><span class="comment"># Flair_Data = shuffle(Flair_Data,random_state = 123)</span></span><br><span class="line"><span class="comment"># T1_Data = shuffle(T1_Data,random_state = 123)</span></span><br><span class="line"><span class="comment"># T1ce_Data = shuffle(T1ce_Data,random_state = 123)</span></span><br><span class="line"><span class="comment"># T2_Data = shuffle(T2_Data,random_state = 123)</span></span><br><span class="line"><span class="comment">#Flair_Data = pd.read_csv('E:\\Data_cjh\\BraTS2019\\radimoic_feature\\2017NET\\flair_select_features.csv')</span></span><br><span class="line"><span class="comment"># T1_Data = pd.read_csv('G:\\MediclImage\\Data\\NeckFibrosis\\new\\T1_PreRT_ok.csv')</span></span><br><span class="line"><span class="comment"># T1ce_Data = pd.read_csv('G:\\MediclImage\\Data\\NeckFibrosis\\new\\T1_C_PreRT_ok.csv')</span></span><br><span class="line"><span class="comment"># T2_Data = pd.read_csv('G:\\MediclImage\\Data\\NeckFibrosis\\new\\T2_PreRT_ok.csv')</span></span><br><span class="line"></span><br><span class="line">Flair_Data.columns = [<span class="string">"&#123;&#125;_flair"</span>.format(Flair_Data.columns[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Flair_Data.columns))]</span><br><span class="line">T1_Data.columns = [<span class="string">"&#123;&#125;_t1"</span>.format(T1_Data.columns[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(T1_Data.columns))]</span><br><span class="line">T1ce_Data.columns = [<span class="string">"&#123;&#125;_t1ce"</span>.format(T1ce_Data.columns[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(T1ce_Data.columns))]</span><br><span class="line">T2_Data.columns = [<span class="string">"&#123;&#125;_t2"</span>.format(T2_Data.columns[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(T2_Data.columns))]</span><br><span class="line"></span><br><span class="line">Multi_Data = pd.concat([Flair_Data,T1_Data,T1ce_Data,T2_Data],axis=1,verify_integrity=False)</span><br><span class="line"></span><br><span class="line">Multi_Data.head()</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选取行列</span></span><br><span class="line">X_Flair_Data = Flair_Data.drop([<span class="string">'ID_flair'</span>,<span class="string">'label_flair'</span>],axis=1)</span><br><span class="line">X_T1_Data = T1_Data.drop([<span class="string">'ID_t1'</span>,<span class="string">'label_t1'</span>],axis=1)</span><br><span class="line">X_T1ce_Data = T1ce_Data.drop([<span class="string">'ID_t1ce'</span>,<span class="string">'label_t1ce'</span>],axis=1)</span><br><span class="line">X_T2_Data = T2_Data.drop([<span class="string">'ID_t2'</span>,<span class="string">'label_t2'</span>],axis=1)</span><br><span class="line">X_Multi_Data = Multi_Data.drop([<span class="string">'ID_flair'</span>,<span class="string">'label_flair'</span>,<span class="string">'ID_t1'</span>,<span class="string">'label_t1'</span>,<span class="string">'ID_t1ce'</span>,<span class="string">'label_t1ce'</span>,<span class="string">'ID_t2'</span>,<span class="string">'label_t2'</span>],axis=1)</span><br><span class="line"></span><br><span class="line">Y = T1ce_Data.loc[:,[<span class="string">'label_t1ce'</span>]]</span><br><span class="line"></span><br><span class="line">X_T2_Data.head()</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 归一化处理</span></span><br><span class="line">import pandas</span><br><span class="line">from sklearn.model_selection import KFold</span><br><span class="line">from sklearn.preprocessing import MinMaxScaler,StandardScaler</span><br><span class="line">from sklearn.svm import SVR</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.preprocessing import LabelEncoder</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment">#scaler = StandardScaler()</span></span><br><span class="line">scaler = MinMaxScaler(feature_range=(0, 1))</span><br><span class="line">X_Flair= scaler.fit_transform(X_Flair_Data)</span><br><span class="line">X_T1 = scaler.fit_transform(X_T1_Data)</span><br><span class="line">X_T1ce = scaler.fit_transform(X_T1ce_Data)</span><br><span class="line">X_T2 = scaler.fit_transform(X_T2_Data)</span><br><span class="line">X_Multi = scaler.fit_transform(X_Multi_Data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sc = StandardScaler()</span></span><br><span class="line"><span class="comment"># X_data = sc.fit_transform(X)</span></span><br><span class="line"></span><br><span class="line">class_labels = LabelEncoder()</span><br><span class="line"></span><br><span class="line">Y_data = class_labels.fit_transform(Y.values.ravel())</span><br></pre></td></tr></table></figure>


<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#方差选择法</span></span><br><span class="line"><span class="comment">#使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用feature_selection库的VarianceThreshold</span></span><br><span class="line">from sklearn.feature_selection import VarianceThreshold</span><br><span class="line">def variance_threshold_selector(data, threshold=0.009):</span><br><span class="line">    selector = VarianceThreshold(threshold)</span><br><span class="line">    selector.fit(data)</span><br><span class="line">    <span class="built_in">return</span> data[data.columns[selector.get_support(indices=True)]]</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征选择方法</span></span><br><span class="line">from sklearn.linear_model import LassoCV, LassoLars, LassoLarsIC,Lasso,ARDRegression,RidgeCV,ElasticNetCV</span><br><span class="line"></span><br><span class="line">def feacture_selection(X_train,Y_train):</span><br><span class="line">    <span class="comment">#regr = ElasticNetCV(l1_ratio=1,max_iter=10000, n_alphas=100,cv=10,normalize =True, random_state=1003,n_jobs=4,tol=0.00001).fit(X_train, Y_train)</span></span><br><span class="line">    regr = ElasticNetCV(l1_ratio=1,cv=5,normalize =True, random_state=1003,n_jobs=4).fit(X_train, Y_train)</span><br><span class="line">    <span class="comment">#regr = LassoCV(cv=5, random_state=0).fit(X_train, Y_train)</span></span><br><span class="line">    reg = regr.coef_</span><br><span class="line">    <span class="comment">#print(reg[reg!=0])</span></span><br><span class="line">    <span class="comment">#print(reg[reg!=0].shape)</span></span><br><span class="line"></span><br><span class="line">    columns = X_train.columns</span><br><span class="line"></span><br><span class="line">    feature_columns = []</span><br><span class="line">    feature_corff = []</span><br><span class="line">    indexs = []</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(X_train.columns.shape[0]):</span><br><span class="line">        <span class="keyword">if</span> reg[index] != 0.0:</span><br><span class="line">            feature_columns.append(columns[index])</span><br><span class="line">            feature_corff.append(reg[index])</span><br><span class="line">    <span class="built_in">return</span> feature_columns</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征选择方法</span></span><br><span class="line">from sklearn.feature_selection import RFE</span><br><span class="line">from sklearn.linear_model import LassoCV</span><br><span class="line">def rfe_lasso(X_train,Y_train,rfe_lasso_num=30):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"rfe_lasso feature start..."</span>)</span><br><span class="line">    linear = LassoCV(alphas=np.logspace(-3, -1, 3))</span><br><span class="line">    rfe = RFE(linear, rfe_lasso_num)</span><br><span class="line">    rfe.fit(X_train, Y_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"rfe_lasso feature end..."</span>)</span><br><span class="line">    <span class="built_in">return</span> X_train.columns[rfe.ranking_].tolist()</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征选择方法</span></span><br><span class="line">def rfe_svm(X_train,Y_train,rfe_svm_num):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"rfe_svm feature start..."</span>)</span><br><span class="line">    from sklearn.feature_selection import RFE</span><br><span class="line">    from sklearn.svm import LinearSVC</span><br><span class="line">    lsvc = LinearSVC(C=0.3, max_iter=50000)</span><br><span class="line">    rfe = RFE(lsvc, rfe_svm_num=30)</span><br><span class="line">    rfe.fit(X_train, Y_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"rfe_svm feature end..."</span>)</span><br><span class="line">    <span class="built_in">return</span> X_train.columns[rfe.ranking_].tolist()</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征选择方法</span></span><br><span class="line">import mifs</span><br><span class="line">def mRMR_feature_select(X_train,Y_train,mRMR_feature_num=30):</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"mRMR feature start..."</span>)</span><br><span class="line">    feat_selector = mifs.MutualInformationFeatureSelector(<span class="string">'MRMR'</span>, n_features=<span class="string">'auto'</span>,n_jobs=2, verbose=1)</span><br><span class="line">    feat_selector.fit(X_train, Y_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"mRMR feature end..."</span>)</span><br><span class="line">    feature_mifs = X_train.columns[feat_selector.ranking_].tolist()</span><br><span class="line">    <span class="built_in">return</span> feature_mifs</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征选择方法</span></span><br><span class="line">from sklearn.ensemble import RandomForestRegressor</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">def feature_selection_rf(X_train,Y_train):</span><br><span class="line">    model = RandomForestRegressor(random_state=1, max_depth=200)</span><br><span class="line">    <span class="comment">#df=pd.get_dummies(X_train)</span></span><br><span class="line">    model.fit(X_train,Y_train.values.ravel())</span><br><span class="line">    features =X_train.columns</span><br><span class="line">    importances = model.feature_importances_</span><br><span class="line">    rf_indices = np.argsort(importances)[-20:]  <span class="comment"># top 10 features</span></span><br><span class="line">    <span class="comment">#indices = np.argsort(importances)[::-1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     for f in range(rf_indices.shape[0]):</span></span><br><span class="line">        <span class="comment">#print("%d. feature %d (%f)" % (f + 1, rf_indices[f], importances[rf_indices[f]]))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     plt.title('Feature Importances')</span></span><br><span class="line"><span class="comment">#     plt.barh(range(len(rf_indices)), importances[rf_indices], color='b', align='center')</span></span><br><span class="line"><span class="comment">#     plt.yticks(range(len(rf_indices)), [features[i] for i in rf_indices])</span></span><br><span class="line"><span class="comment">#     plt.xlabel('Relative Importance')</span></span><br><span class="line"><span class="comment">#     plt.show()</span></span><br><span class="line">    rf_features = [features[i] <span class="keyword">for</span> i <span class="keyword">in</span> rf_indices]</span><br><span class="line">    <span class="built_in">return</span> rf_features</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 评估指标</span></span><br><span class="line">from sklearn.metrics import confusion_matrix,classification_report,roc_auc_score</span><br><span class="line">def evalution_metirc(y_test,y_pred_1,y_pred,labels,target_names):</span><br><span class="line"></span><br><span class="line">    auc = roc_auc_score(y_test, y_pred_1)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'ROC AUC: %f'</span> % auc)</span><br><span class="line">    confusion = confusion_matrix(y_test, y_pred)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">float</span>(np.sum(confusion)) != 0:</span><br><span class="line">        accuracy = <span class="built_in">float</span>(confusion[0, 0] + confusion[1, 1]) / <span class="built_in">float</span>(np.sum(confusion))</span><br><span class="line">    <span class="comment">#print("Global Accuracy: " + str(accuracy))</span></span><br><span class="line">    specificity = 0</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">float</span>(confusion[0, 0] + confusion[0, 1]) != 0:</span><br><span class="line">        specificity = <span class="built_in">float</span>(confusion[0, 0]) / <span class="built_in">float</span>(confusion[0, 0] + confusion[0, 1])</span><br><span class="line">    <span class="comment">#print("Specificity: " + str(specificity))</span></span><br><span class="line">    sensitivity = 0</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">float</span>(confusion[1, 1] + confusion[1, 0]) != 0:</span><br><span class="line">        sensitivity = <span class="built_in">float</span>(confusion[1, 1]) / <span class="built_in">float</span>(confusion[1, 1] + confusion[1, 0])</span><br><span class="line">    <span class="comment">#print("Sensitivity: " + str(sensitivity))</span></span><br><span class="line">    <span class="comment"># make predictions for test data and evaluate</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(confusion)</span><br><span class="line">    <span class="built_in">print</span>(classification_report(y_test, y_pred, labels=labels, target_names=target_names))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> auc,accuracy,specificity,sensitivity</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">类别不平衡问题，顾名思义，即数据集中存在某一类样本，其数量远多于或远少于其他类样本，从而导致一些机器学习模型失效的问题。例如逻辑回归即不适合处理类别不平衡问题，例如逻辑回归在欺诈检测问题中，因为绝大多数样本都为正常样本，欺诈样本很少，逻辑回归算法会倾向于把大多数样本判定为正常样本，这样能达到很高的准确率，但是达不到很高的召回率。</span><br><span class="line"></span><br><span class="line">       类别不平衡问题在很多场景中存在，例如欺诈检测，风控识别，在这些样本中，黑样本（一般为存在问题的样本）的数量一般远少于白样本（正常样本）。</span><br><span class="line"></span><br><span class="line">       上采样(过采样)和下采样(负采样）策略是解决类别不平衡问题的基本方法之一。上采样即增加少数类样本的数量，下采样即减少多数类样本以获取相对平衡的数据集。</span><br><span class="line"></span><br><span class="line">       最简单的上采样方法可以直接将少数类样本复制几份后添加到样本集中，最简单的下采样则可以直接只取一定百分比的多数类样本作为训练集。</span><br><span class="line"></span><br><span class="line">       SMOTE算法是用的比较多的一种上采样算法，SMOTE算法的原理并不是太复杂，用python从头实现也只有几十行代码，但是python的imblearn包提供了更方便的接口，在需要快速实现代码的时候可直接调用imblearn。</span><br><span class="line"></span><br><span class="line">       imblearn类别不平衡包提供了上采样和下采样策略中的多种接口，基本调用方式一致，主要介绍一下对应的SMOTE方法和下采样中的RandomUnderSampler方法。imblearn可使用pip install imblearn直接安装。</span><br><span class="line"></span><br><span class="line">from imblearn.over_sampling import SMOTE</span><br><span class="line">def smote_augment_data(X_train,y_train):</span><br><span class="line">    x_columns = X_train.columns</span><br><span class="line">    y_columns = y_train.columns</span><br><span class="line">    <span class="comment">#注意过采样时 只对训练集进行过采样</span></span><br><span class="line">    oversampler=SMOTE(random_state=0)</span><br><span class="line">    x_smote_train,y_smote_train=oversampler.fit_sample(X_train,y_train)</span><br><span class="line">    x_smote_train = pd.DataFrame(x_smote_train, columns=x_columns)</span><br><span class="line">    y_smote_train = pd.DataFrame(y_smote_train, columns=y_columns)</span><br><span class="line">    <span class="built_in">return</span> x_smote_train,y_smote_train</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 深度学习模型</span></span><br><span class="line">from keras.models import *</span><br><span class="line">from keras.layers import *</span><br><span class="line">from keras.optimizers import Adam</span><br><span class="line">def deep_learning_model(X_train,y_train,n_dim):</span><br><span class="line">    model = Sequential()</span><br><span class="line">    model.add(Dense(512, activation=<span class="string">'relu'</span>,input_dim = n_dim))</span><br><span class="line">    model.add(BatchNormalization())</span><br><span class="line">    model.add(Dropout(0.2))</span><br><span class="line">    model.add(Dense(1, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">    model.compile(optimizer =Adam(0.001),loss=<span class="string">'binary_crossentropy'</span>, metrics =[<span class="string">'accuracy'</span>])</span><br><span class="line">    model.fit(X_train,y_train, batch_size=10, epochs=25,verbose=0)</span><br><span class="line">    <span class="built_in">return</span> model</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 深度学习模型</span></span><br><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line">from sklearn.linear_model.logistic import LogisticRegression</span><br><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line">from xgboost import XGBClassifier</span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line">def grid_search_cv(model,param_grid,X,Y):</span><br><span class="line">    grid_search = GridSearchCV(model,param_grid,scoring=<span class="string">'roc_auc'</span>,n_jobs=2,verbose=1,cv=5)</span><br><span class="line">    grid_search.fit(X,Y)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"&#123;&#125;:&#123;&#125;"</span>.format(model.__class__.__name__,grid_search.best_estimator_))</span><br><span class="line">    <span class="built_in">return</span> grid_search.best_estimator_.get_params()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def lr_model(x,y):</span><br><span class="line">    model = LogisticRegression(random_state=25)</span><br><span class="line">    param_grid = &#123;<span class="string">'max_iter'</span>:range(20,151,10),<span class="string">'C'</span>:[1e-3,1e-2,1e-1,1],<span class="string">'penalty'</span>:[<span class="string">"l1"</span>,<span class="string">"l2"</span>]&#125;</span><br><span class="line">    best_parameters = grid_search_cv(model,param_grid,x,y)</span><br><span class="line">    model = LogisticRegression(random_state=25,max_iter=best_parameters[<span class="string">'max_iter'</span>],C=best_parameters[<span class="string">'C'</span>],</span><br><span class="line">                               penalty=best_parameters[<span class="string">'penalty'</span>],class_weight=<span class="string">'balanced'</span>)</span><br><span class="line">    model.fit(x,y)</span><br><span class="line">    <span class="built_in">return</span> model</span><br><span class="line"></span><br><span class="line">def rf_model(x,y):</span><br><span class="line">    model = RandomForestClassifier(random_state=25,n_jobs =2)</span><br><span class="line">    param_grid = &#123;<span class="string">'n_estimators'</span>:range(50,301,10),<span class="string">'max_depth'</span>:range(3,14,2),</span><br><span class="line">                  <span class="string">'min_samples_split'</span>:range(50,201,20),<span class="string">'min_samples_leaf'</span>:range(10,60,10),</span><br><span class="line">                  <span class="string">'max_features'</span>:(<span class="string">'auto'</span>,<span class="string">'log2'</span>,None)&#125;</span><br><span class="line">    best_parameters = grid_search_cv(model,param_grid,x,y)</span><br><span class="line">    model = RandomForestClassifier(random_state=25,n_jobs=2,n_estimators=best_parameters[<span class="string">'n_estimators'</span>],</span><br><span class="line">                                  max_depth=best_parameters[<span class="string">'max_depth'</span>],min_samples_split=best_parameters[<span class="string">'min_samples_split'</span>],</span><br><span class="line">                                  min_samples_leaf=best_parameters[<span class="string">'min_samples_leaf'</span>],max_features=best_parameters[<span class="string">'max_features'</span>])</span><br><span class="line">    model.fit(x,y)</span><br><span class="line">    <span class="built_in">return</span> model</span><br><span class="line"></span><br><span class="line">def svm_model(x,y):</span><br><span class="line">    model = SVC(random_state=25,n_jobs=2)</span><br><span class="line">    param_grid = &#123;<span class="string">'kernel'</span>:(<span class="string">'linear'</span>,<span class="string">'rbf'</span>),<span class="string">'C'</span>:(0.1,0.25,0.5,0.75,1,10,100),</span><br><span class="line">                  <span class="string">'gamma'</span>:(1,0.1,0.01,0.001,0.0001,<span class="string">'auto'</span>),<span class="string">'shrinking'</span>:(True,False),</span><br><span class="line">                 <span class="string">'decision_function_shape'</span>:(<span class="string">'ovo'</span>,<span class="string">'ovr'</span>)&#125;</span><br><span class="line"></span><br><span class="line">    best_parameters = grid_search_cv(model,param_grid,x,y)</span><br><span class="line">    model = SVC(random_state=25,n_jobs=2,kernel=best_parameters[<span class="string">'kernel'</span>],C=best_parameters[<span class="string">'C'</span>],gamma=best_parameters[<span class="string">'gamma'</span>],</span><br><span class="line">               shrinking=best_parameters[<span class="string">'shrinking'</span>],decision_function_shape=best_parameters[<span class="string">'decision_function_shape'</span>])</span><br><span class="line">    model.fit(x,y)</span><br><span class="line">    <span class="built_in">return</span> model</span><br><span class="line"></span><br><span class="line">def xgb_model(x,y):</span><br><span class="line">    model = XGBClassifier(random_state=25,n_jobs=2)</span><br><span class="line">    param_grid = &#123;<span class="string">'min_child_weight'</span>:range(1,11,2),<span class="string">'colsample_bytree'</span>:range(0.2,1.1,0.2),</span><br><span class="line">                  <span class="string">'gamma'</span>:range(0.25,5,0.25),<span class="string">'max_depth'</span>:range(2,11,1)&#125;</span><br><span class="line"></span><br><span class="line">    best_parameters = grid_search_cv(model,param_grid,x,y)</span><br><span class="line">    model = XGBClassifier(random_state=25,n_jobs=2,min_child_weight=best_parameters[<span class="string">'min_child_weight'</span>],</span><br><span class="line">                          gamma=best_parameters[<span class="string">'gamma'</span>],colsample_bytree=best_parameters[<span class="string">'colsample_bytree'</span>],</span><br><span class="line">                         max_depth=best_parameters[<span class="string">'max_depth'</span>])</span><br><span class="line">    model.fit(x,y)</span><br><span class="line">    <span class="built_in">return</span> model</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def covert_to_dataframe(train_index,test_index,X_Flair,Y_data,X_Flair_Data):</span><br><span class="line">    X_Falir_train, X_Flair_test, y_Flair_train, y_Flair_test  = X_Flair[train_index], X_Flair[test_index], Y_data[train_index], Y_data[test_index]</span><br><span class="line"></span><br><span class="line">    X_Falir_train = pd.DataFrame(X_Falir_train, columns=X_Flair_Data.columns)</span><br><span class="line">    y_Flair_train = pd.DataFrame(y_Flair_train, columns=Y.columns)</span><br><span class="line"></span><br><span class="line">    X_Flair_test = pd.DataFrame(X_Flair_test, columns=X_Flair_Data.columns)</span><br><span class="line">    y_Flair_test = pd.DataFrame(y_Flair_test, columns=Y.columns)</span><br><span class="line">      <span class="comment">#去除0方差变量</span></span><br><span class="line">    X_Falir_train = variance_threshold_selector(X_Falir_train,0.0)</span><br><span class="line">    X_Flair_test = X_Flair_test[X_Falir_train.columns]</span><br><span class="line">    <span class="built_in">return</span> X_Falir_train, X_Flair_test, y_Flair_train, y_Flair_test</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">lgr = LogisticRegression()</span><br><span class="line">rf = RandomForestClassifier(n_jobs=2, random_state=25,n_estimators=100)</span><br><span class="line">svm = SVC(kernel=<span class="string">'linear'</span>,probability=True,degree=3, gamma=1.0) <span class="comment">#linear</span></span><br><span class="line">xgb = XGBClassifier(learning_rate=0.3, max_depth=4, eta=0.2)</span><br><span class="line">deep = None</span><br><span class="line">models = [lgr,rf,svm]</span><br><span class="line"></span><br><span class="line">n_fold = 5</span><br><span class="line"></span><br><span class="line">AUC_train = np.zeros((3, n_fold), dtype=np.float)</span><br><span class="line">Accuracy_train = np.zeros((3, n_fold), dtype=np.float)</span><br><span class="line">Specificity_train = np.zeros((3, n_fold), dtype=np.float)</span><br><span class="line">Sensitivity_train = np.zeros((3, n_fold), dtype=np.float)</span><br><span class="line">AUC_test =  np.zeros((3, n_fold), dtype=np.float)</span><br><span class="line">Accuracy_test = np.zeros((3, n_fold), dtype=np.float)</span><br><span class="line">Specificity_test =  np.zeros((3, n_fold), dtype=np.float)</span><br><span class="line">Sensitivity_test =  np.zeros((3, n_fold), dtype=np.float)</span><br><span class="line"></span><br><span class="line">kFold = KFold(n_splits=n_fold, random_state=42, shuffle=True)</span><br><span class="line">total_features = []</span><br><span class="line">i = 0</span><br><span class="line"><span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> kFold.split(X_Flair,Y_data):</span><br><span class="line">    <span class="comment">#print("Train Index: ", train_index, "\n")</span></span><br><span class="line">    <span class="comment">#print("Test Index: ", test_index)</span></span><br><span class="line"></span><br><span class="line">    X_Flair_train, X_Flair_test, y_Flair_train, y_Flair_test =covert_to_dataframe(train_index,test_index,X_Flair,Y_data,X_Flair_Data)</span><br><span class="line">    X_T1_train, X_T1_test, y_T1_train, y_T1_test = covert_to_dataframe(train_index,test_index,X_T1,Y_data,X_T1_Data)</span><br><span class="line">    X_T1ce_train, X_T1ce_test, y_T1ce_train, y_T1ce_test =covert_to_dataframe(train_index,test_index,X_T1ce,Y_data,X_T1ce_Data)</span><br><span class="line">    X_T2_train, X_T2_test, y_T2_train, y_T2_test =covert_to_dataframe(train_index,test_index,X_T2,Y_data,X_T2_Data)</span><br><span class="line">   <span class="comment"># X_Multi_train, X_Multi_test, y_Multi_train, y_Multi_test =covert_to_dataframe(train_index,test_index,X_Multi,Y_data,X_Multi_Data)</span></span><br><span class="line"><span class="comment">#     print("y_Flair_train:",np.array(y_Flair_train))</span></span><br><span class="line"><span class="comment">#     print("y_T1_train:",np.array(y_T1_train))</span></span><br><span class="line">    <span class="comment">#assert y_Flair_train == y_T1_train,'两个集合不等 '</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">####特征选择方法</span></span><br><span class="line">    flair_feature_name = feacture_selection(X_Flair_train,y_Flair_train)</span><br><span class="line">    t1_feature_name = feacture_selection(X_T1_train,y_T1_train)</span><br><span class="line">    t1ce_feature_name = feacture_selection(X_T1ce_train,y_T1ce_train)</span><br><span class="line">    t2_feature_name = feacture_selection(X_T2_train,y_T2_train)</span><br><span class="line"></span><br><span class="line"><span class="comment">#     Multi_feature_name = feacture_selection(X_Multi_train,y_Multi_train)</span></span><br><span class="line"></span><br><span class="line">    total_Train_Data = pd.concat([X_Flair_train[flair_feature_name],X_T1_train[t1_feature_name],X_T1ce_train[t1ce_feature_name],X_T2_train[t2_feature_name]],axis=1,verify_integrity=False)</span><br><span class="line">    total_Test_Data = pd.concat([X_Flair_test[flair_feature_name],X_T1_test[t1_feature_name],X_T1ce_test[t1ce_feature_name],X_T2_test[t2_feature_name]],axis=1,verify_integrity=False)</span><br><span class="line"></span><br><span class="line"><span class="comment">#     total_Train_Data = X_Multi_train[Multi_feature_name]</span></span><br><span class="line"><span class="comment">#     total_Test_Data =  X_Multi_test[Multi_feature_name]</span></span><br><span class="line"></span><br><span class="line">    feature_name = rfe_lasso(total_Train_Data,y_Flair_train)</span><br><span class="line"><span class="comment">#     total_features.append(Multi_feature_name)</span></span><br><span class="line"><span class="comment">#     feature_name = Multi_feature_name</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"the number of feature is "</span>,len(feature_name),<span class="string">"\n"</span>,feature_name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> len(feature_name)&gt;0:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> index,train_model <span class="keyword">in</span> enumerate(models):</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"++++++++++++++++++the &#123;&#125;th model+++++++++"</span>.format(index+1))</span><br><span class="line"></span><br><span class="line">            x_smote_train,y_smote_train = smote_augment_data(total_Train_Data[feature_name],y_Flair_train)</span><br><span class="line">            model = train_model.fit(x_smote_train,y_smote_train)</span><br><span class="line"><span class="comment">#             if index==0:</span></span><br><span class="line"><span class="comment">#                 model = lr_model(x_smote_train,y_smote_train)</span></span><br><span class="line"><span class="comment">#             elif index ==1:</span></span><br><span class="line"><span class="comment">#                 model = rf_model(x_smote_train,y_smote_train)</span></span><br><span class="line"><span class="comment">#             elif index ==2:</span></span><br><span class="line"><span class="comment">#                 model = svm_model(x_smote_train,y_smote_train)</span></span><br><span class="line"><span class="comment">#             elif index ==3:</span></span><br><span class="line"><span class="comment">#                  model = xgb_model(x_smote_train,y_smote_train)</span></span><br><span class="line"></span><br><span class="line">            y_train_pred_class = model.predict(total_Train_Data[feature_name])</span><br><span class="line">            y_train_pred = model.predict_proba(total_Train_Data[feature_name])[:,1]</span><br><span class="line"></span><br><span class="line">            y_test_pred_class = model.predict(total_Test_Data[feature_name])</span><br><span class="line">            y_test_pred = model.predict_proba(total_Test_Data[feature_name])[:,1]</span><br><span class="line"></span><br><span class="line">            auc,accuracy,specificity,sensitivity = evalution_metirc(y_Flair_train,y_train_pred,y_train_pred_class,labels=[0,1],target_names=[<span class="string">"LGG"</span>,<span class="string">"HGG"</span>])</span><br><span class="line">            AUC_train[index,i] = auc</span><br><span class="line">            Accuracy_train[index,i] = accuracy</span><br><span class="line">            Specificity_train[index,i] = specificity</span><br><span class="line">            Sensitivity_train[index,i] = sensitivity</span><br><span class="line"></span><br><span class="line">            <span class="comment">### 测试集</span></span><br><span class="line">            auc_1,accuracy_1,specificity_1,sensitivity_1 = evalution_metirc(y_Flair_test,y_test_pred,y_test_pred_class,labels=[0,1],target_names=[<span class="string">"LGG"</span>,<span class="string">"HGG"</span>])</span><br><span class="line">            AUC_test[index,i] = auc_1</span><br><span class="line">            Accuracy_test[index,i] = accuracy_1</span><br><span class="line">            Specificity_test[index,i] = specificity_1</span><br><span class="line">            Sensitivity_test[index,i] = sensitivity_1</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"the  number of feature is 0. "</span>)</span><br><span class="line">    i=i+1</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(0,3,1):</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"++++++++++++++++++++++the &#123;&#125;th model+++++++++++++++"</span>.format(i))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"###############训练集####################"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"AUC:"</span>,AUC_train[i,:],<span class="string">"average:"</span>,np.mean(AUC_train[i,:]),<span class="string">"std:"</span>,np.std(AUC_train[i,:]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Accuracy:"</span>,Accuracy_train[i,:],<span class="string">"average:"</span>,np.mean(Accuracy_train[i,:]),<span class="string">"std:"</span>,np.std(Accuracy_train[i,:]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Specificity:"</span>,Specificity_train[i,:],<span class="string">"average:"</span>,np.mean(Specificity_train[i,:]),<span class="string">"std:"</span>,np.std(Specificity_train[i,:]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Sensitivity:"</span>,Sensitivity_train[i,:],<span class="string">"average:"</span>,np.mean(Sensitivity_train[i,:]),<span class="string">"std:"</span>,np.std(Sensitivity_train[i,:]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"###############测试集####################"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"AUC:"</span>,AUC_test[i,:],<span class="string">"average:"</span>,np.mean(AUC_test[i,:]),<span class="string">"std:"</span>,np.std(AUC_test[i,:]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Accuracy:"</span>,Accuracy_test[i,:],<span class="string">"average:"</span>,np.mean(Accuracy_test[i,:]),<span class="string">"std:"</span>,np.std(Accuracy_test[i,:]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Specificity:"</span>,Specificity_test[i,:],<span class="string">"average:"</span>,np.mean(Specificity_test[i,:]),<span class="string">"std:"</span>,np.std(Specificity_test[i,:]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Sensitivity:"</span>,Sensitivity_test[i,:],<span class="string">"average:"</span>,np.mean(Sensitivity_test[i,:]),<span class="string">"std:"</span>,np.std(Sensitivity_test[i,:]))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model_s = [<span class="string">'lgr'</span>,<span class="string">'rf'</span>,<span class="string">'svm'</span>,<span class="string">'xgb'</span>]</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">'model_s'</span>: model_s,<span class="string">'AUC_train'</span>:np.mean(AUC_train,axis=1),<span class="string">'Accuracy_train'</span>:np.mean(Accuracy_train,axis=1),<span class="string">'Specificity_train'</span>: np.mean(Specificity_train,axis=1),</span><br><span class="line">                   <span class="string">'Sensitivity_train'</span>:np.mean(Specificity_train,axis=1),<span class="string">'AUC_test'</span>:np.mean(AUC_test,axis=1),<span class="string">'Accuracy_test'</span>:np.mean(Accuracy_test,axis=1),<span class="string">'Specificity_test'</span>: np.mean(Specificity_test,axis=1),</span><br><span class="line">                   <span class="string">'Sensitivity_test'</span>:np.mean(Sensitivity_test,axis=1)&#125;)</span><br><span class="line">df.to_csv(<span class="string">'mac_2019_multi_Modality_3_standpro_result.csv'</span>)</span><br><span class="line">total_features</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model_s = [<span class="string">'lgr'</span>,<span class="string">'rf'</span>,<span class="string">'svm'</span>,<span class="string">'xgb'</span>]</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">'AUC_train'</span>:AUC_train[3,:],<span class="string">'Accuracy_train'</span>:Accuracy_train[3,:],<span class="string">'Specificity_train'</span>: Specificity_train[3,:],</span><br><span class="line">                   <span class="string">'Sensitivity_train'</span>:Specificity_train[3,:],<span class="string">'AUC_test'</span>:AUC_test[3,:],<span class="string">'Accuracy_test'</span>:Accuracy_test[3,:],<span class="string">'Specificity_test'</span>:Specificity_test[3,:],</span><br><span class="line">                   <span class="string">'Sensitivity_test'</span>:Sensitivity_test[3,:]&#125;)</span><br><span class="line">df.to_csv(<span class="string">'mac_T1ce__modality.csv'</span>)</span><br><span class="line">total_features</span><br></pre></td></tr></table></figure>
</div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Wuli帅林林</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="http://yoursite.com/2019/11/08/glioma/">http://yoursite.com/2019/11/08/glioma/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yoursite.com">Welcome to linlin</a>！</span></div></div></article><div id="pagination"><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2019/11/05/%E6%9C%AC%E5%91%A8%E8%AE%A1%E5%88%92/"><span>本周计划</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2019 By Wuli帅林林</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--></body></html>